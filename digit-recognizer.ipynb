{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer (MNIST Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is heavily based on Jeremy Howard's notebooks from Practical Deep Learning For Coders, Part 1 (course.fast.ai). Download the competitions files into data/mnist folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Competition: https://www.kaggle.com/c/digit-recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: 0.994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=\"data/mnist/\"\n",
    "model_path = path + 'models/'\n",
    "results_path = path + 'results/'\n",
    "submissions_path = path + 'submissions/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "if not os.path.exists(results_path): os.mkdir(results_path)\n",
    "if not os.path.exists(submissions_path): os.mkdir(submissions_path)\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv(path+'train.csv')\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(path+\"test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = training['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 7, 6, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = training.iloc[:,1:].values\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) #reshape to rectangular\n",
    "X_train = X_train/255 #pixel values are 0 - 255 - this makes puts them in the range 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = X_train[32000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[0:32000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_val = Y_train[32000:]\n",
    "Y_train = Y_train[0:32000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = test.iloc[:,0:].values\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1) #reshape to rectangular\n",
    "X_test = X_test/255 #pixel values are 0 - 255 - this makes puts them in the range 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32000, 28, 28, 1), (32000,), (10000, 28, 28, 1), (10000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the axis that theano expects (channel on axis 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train,1)\n",
    "X_val = np.expand_dims(X_val,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.squeeze(X_train,4)\n",
    "X_val = np.squeeze(X_val,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32000, 1, 28, 28), (32000,), (10000, 1, 28, 28), (10000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Y_train = onehot(Y_train)\n",
    "Y_val = onehot(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def norm_input(x): return (x-mean_px)/std_px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plots(ims, interp=False, titles=None):\n",
    "    ims=np.array(ims)\n",
    "    mn,mx=ims.min(),ims.max()\n",
    "    f = plt.figure(figsize=(12,24))\n",
    "    for i in range(len(ims)):\n",
    "        sp=f.add_subplot(1, len(ims), i+1)\n",
    "        if not titles is None: sp.set_title(titles[i], fontsize=18)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none', vmin=mn,vmax=mx)\n",
    "\n",
    "def plot(im, interp=False):\n",
    "    f = plt.figure(figsize=(3,6), frameon=True)\n",
    "    plt.imshow(im, interpolation=None if interp else 'none')\n",
    "    \n",
    "plt.gray()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = np.squeeze(X_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 28, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA80AAACkCAYAAACtpcz6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmYFMX9P/D3J5zCiiC3eCwqeOOBC/IVjArxwARvA1Ej\najSCRo2KkV8SQROjYryioiEaTyAxioqGQy7D4QGoeIOiLIocggcgyF2/P2amqSq2Z+fonu6ueb+e\nZx+qp3p6in1v9WzvVHWJUgpEREREREREtKMfRd0AIiIiIiIiorjiRTMRERERERGRD140ExERERER\nEfngRTMRERERERGRD140ExEREREREfngRTMRERERERGRD140ExEREREREfngRTMRERERERGRD140\nExEREREREfngRTMRERERERGRD140F0BEdhKRk0Tk9yLyrIhUi8i29NeNUbePiiciFSIyVETeFZG1\nIvKdiMwWkWtEpF7U7aP8sd+WFxG5Qct3a9TtocLxfOwuZusW7Zyby9eUqNtL+RGRw0XkRhF5QUQ+\nEpFVIrIp/e9MEfl/ItIs6naGpW7UDUioLgDGpcvK+pcSTkT2AvAKgL2QynU9gPoAOgM4EsC5InK8\nUmp1ZI2kQrDflgkR2Q/AjUjlKxE3h4rA87G7mK2TltdSXw/Aruny7JDbQsG7CMDl2P670wak+m0z\nAN0A/B+Aq0Wkj1Lq9WiaGB5+0ly4bwBMBnAHgH4AVkTbHAqCiNQB8CJSb+JLAfRSSu2slGoEoC+A\nNQAOA/BUdK2kIrDfOk5EBMA/ATQA8FrEzaEi8HzsLmbrJqXUbtm+ANyK7X/I/GeETaXCvAHgOqQu\nkJsppRorpZoC2BnABQC+AtACwHMisnN0zQwHP2kuzHSlVAv9ARG5ParGUKD6AzgYqb+inaGU8v4S\nqpT6T/qNfhSA3iJynFJqWjTNpAKw35aHK5F6Q38SwGdI/eWbkqk/eD52VX8w23L0K6Qyn6mU+iTq\nxlB+lFI1/hFLKbUewFMisgLARACtAPwUwOgSNi90/KS5AEopDul01y+ROqFP09/EM5RS/wKwSNuX\nEoL91n0i0h7AnwGsBHBNxM2h4vF87C5mW2ZEpBuA/dOb/4iyLRQafUj27pG1IiS8aCZKE5GdAByd\n3hyfZdcJSA0vOiH0RhFRPv4BoBGAa5RSX0fdGCocz8fuYrZl61fpf1cDeCbKhlBojtHKn0bWipDw\noplouwOwvU+8n2W/TF0bEWkabpOIKBcicgmA4wFMUkqNjLo9VDSej93FbMuMiDQGcDZSowtGKaU2\nRNwkCoiI1BeRvUTkCgBPpB/+BKl7FjiFc5qJtttNK3+ZZT+9bjcA34XTHCLKhYjsBmAYUnfxvCzi\n5lAweD52F7MtP/0AVCB10fxIxG2hAIjIBqTudq9TAGYA+IVSanPpWxUuXjQTbaff6W99lv30Oufu\nDkiUQCMANAFwvVKqOuK2UDB4PnYXsy0/F6f/fUcp9XakLaGgLAXQEKk/hjROPzYNqffhbH8MSywO\nzyYiosQSkfMA9AbwNoC7I24OERFpRORAAF2R+hSSNwBzhFJq7/RSYk0AtEZqKarDAcwRkZuibV04\neNFMtN1ardwoy3563VrfvYgoVCLSCsA9ALYAuEQptS3iJlFweD52F7MtL5ek/90AgPebcJBSapVS\n6m4AJyH1x5E/iEjviJsVOF40E223VCu3y7KfXrfUdy8iCtvtAJohNTz7YxFprH9Bm2+lPV4vqsZS\nXng+dhezLRPp8+25SF1IPaOUWhNxkyhESqk5AGamNy+Nsi1h4EUz0XYfAch8UnVwlv0ydcuVUrwx\nCVF0KpFakmYgUp9E2V+DtX0zj91e2iZSgXg+dhezLR+nAmiRLvMGYOXhS6Tel/eNuiFB40UzUZpS\n6gcAs5Dq7Cdl2fVEpP5q+nIp2kVEWalavmraj2KO52N3MduyklmbeaFSanqkLaFS2Tv9r3NTKnjR\nTGR6PP3vcSJSZVeKyDnYfkJ4wq4notJRSh2nlKrj9wXgZm3fzOPXRthkyg/Px+5ito4TkT0A9AKX\nmXKCiNR6zSgiPQF0QSrzaaE3qsR40VwgEWkqIs3TXy2w/XvZSHu8eXpeHSXH4wDeQyrPMSJyPABI\nytlIzZ1UAMYppZw7IbiO/ZYoUXg+dhezdd/FSOW7Bdv/SELJtYeIvC0il4pIe71CRHYXkRsAPI/U\nCJKvkbpJp1NEKY5UK4SIVAPYM4ddH1NKXRRycyhAIrIXgKnYPl9yPVIn/oZIvYm/BaCXUmp1VG2k\nwrDflhcRGQJgCACV/uSZEobnY3cxW3eJiACoBrAHgOeVUmdE2yIqVrq/LtIe2gRgDYCdsH2dZgXg\nMwBnKqXeLW0Lw8dPmgu3DbXPpeP8uQRSSi0G0AmpoZ3vIZX1JgBzAVwLoBvfxBOL/bb8MM8E4/nY\nXczWab0A7I5Upg9H3BYKxlIAZwG4H8AcACsB7IzUH7wWAxiL1OiCg128YAb4STMRERERERGRL37S\nTEREREREROSDF81EREREREREPkK7aBaRy0VkkYj8ICKv17SkACUPc3UXs3UXs3UXs3UTc3UXs3UX\ns3VbKBfNIvJzAHciddfSwwG8A2BieokXSijm6i5m6y5m6y5m6ybm6i5m6y5m675QbgQmIq8DeEMp\ndVV6WwB8AeBvSqlhgb8glQRzdRezdRezdRezdRNzdRezdRezdV/doA8oIvUAdAbwl8xjSiklIpMB\ndKth/+YATkRqPbcNQbeH8tIQqfUSJyqlvtYr8s01/RxmGx/M1l2BZctcY8U3V4DZJhzPx+5itm7i\n+dhdWbPVBX7RDKAFgDoAVliPrwCwXw37nwhgZAjtoMKdC2CU9Vi+uQLMNo6YrbuCyJa5xk9NuQLM\n1gU8H7uL2bqJ52N3+WXricPds6ujbgDtoDpmx6EiNWjQIFOsDuiQQR2HitSyZctMsTqAwwVxDApW\ndcyOQ8GpjtlxqEgVFRWZYnVAhwzqOBSM6pgdh4JTXdsOYVw0rwKwFUBr6/HWAJbXsD+HJcRPTZnk\nm6vfcSgCbdu2zRSZrWN69eqVKQaRLXONH79MmG3y8XzsmAMOOCBTZLZu4vnYXbVmEvhFs1JqM4A3\nAfTMPJaeDN8TwKtBvx6VBnN1F7N1F7N1F7N1E3N1F7N1F7MtD2HMaQaAuwA8JiJvApgN4LcAGgF4\nLKTXo9Jgru5itu5itu5itm5iru5itu5ito4L5aJZKfV0el2ym5EamjAPwIlKqZVhvB6VBnN1F7N1\nF7N1F7N1E3N1F7N1F7N1X1ifNEMpNRzA8LCOT9Fgru5itu5itu5itm5iru5itu5itm6Lw92ziYiI\niIiIiGKJF81EREREREREPnjRTEREREREROSDF81EREREREREPnjRTEREREREROSDF81ERERERERE\nPgK/aBaRISKyzfr6MOjXodJjtm5iru5itu5itu5itm5iru5ituUhrHWa3wfQE4Ckt7eE9DrOmzRp\nklc+/vjjjbr+/ft75SeffLJUTUpsts2aNTO2d955Z688cOBA3+d16dLF2H7wwQe98po1a4y6iRMn\nFtPEKCU219rUqVPH2L799tu98tatW426wYMHe+Vt27aF27DScTZbSla2IuKV27RpY9RddtllXrlt\n27ZG3cUXX+x7TKWUsf3oo4965ZtvvtmoW7Jkie/zYihR2eYq2/m4R48eRl3nzp298owZM4y6K664\nwit/8MEHQTYxbE7mSgCYrfPCumjeopRaGdKxKVrM1k3M1V3M1l3M1l3M1k3M1V3M1nFhzWnuICJf\nisinIvKUiOwR0utQ6TFbNzFXdzFbdzFbdzFbNzFXdzFbx4XxSfPrAPoDWACgLYChAKaLyMFKqXUh\nvJ5Tpk6damwfffTRXtkeThbB8LLYZ6sPuQaAk08+2SvbQ9jr1i3sx3+33XbzyrvvvrtR9/jjjxvb\nw4YN88qLFy8u6PVKIPa5FqN+/frG9m9/+1uvbPehG2+80Stv3Lgx3IaVRmyy/fjjj73yRx99ZNSd\nddZZXnnz5s0laxMA7LTTTsZ2z549vfJLL71U0rbkKTbZ+mnYsKGxfcEFF3jlBx54IJDX0Id8A8CF\nF17olfUpTAAwaNAgr3zvvfcadTEbrh37bPNRr149r6wPnweAfv36eeVx48YZdaNHj/bKZ599tlH3\n6quveuVzzjnHqIvxNCmnciUDsy0DgV80K6X0s9X7IjIbwGIA5wB4tOZnURIwWzcxV3cxW3cxW3cx\nWzcxV3cx2/IQ+pJTSqnVAD4GsG/Yr0WlxWyTY8WKFTnvy1yTZfr06Tnvy2zdxWzdxWyT45NPPsl5\nX+bqLmbrptAvmkWkAsA+AJaF/VpUWsw2OVq3bp3zvsw1WY455pic92W27mK27mK2ydGhQ4ec92Wu\n7mK2bgp8eLaI3AHgRaSGJbQDcBNSt10fne155ez3v/+9V+7WrZtRpy/P8PTTTxt1Y8aMCbdhlrhm\n27RpU6/8xBNPGHWnnHKK7/PsuXC5yvamqC+bAgCnnXZajWUAWLBggVe2l64qpbjmGpZCc0+iOGWr\nzxWeP3++Ude4cWOv/N1335WsTQCw6667Gtt/+MMfvHKc5zTHKVudnuXMmTONukMOOcQrh9UP9ePa\nr3HHHXd45U2bNhl1w4cPD6U9hYhrtoW66aabvLI+hxkAHnroIa98+eWXG3V6fvq9RADg2GOP9cr2\n70b6z9nnn3+ef4ND4lqutF0Ss23ZsqWxrS/j1r17d6NO72/2/R+2bNm+spZ9XwL9vV7/ndf2/PPP\nG9vff/+9V7aXBo1SGDcC2x3AKADNAawEMBPAUUqpr0N4LSotZusm5uouZusuZusuZusm5uouZlsG\nwrgRWL/a96IkYrZuYq7uYrbuYrbuYrZuYq7uYrblIYxPmqkW9jBdfXi2vjQDALz33nte+de//rVR\nt379+hBalzxHHXWUV7aHY4cxBDCfY7Zp08Yrv/baa0adPhTt73//e/ENo6Lpy9XowwapeEuWLPHK\n+nAuALj99tu9sn2eK7WqqiqvbM8Xz+ema+WqRYsWXlkfJgtEPzVCf319KCJgDtf+5z//adRt27Yt\n3IY55vTTTze29WX+9N9pAODqq6/2PY4+DHTp0qVG3bfffuuVmzVrZtTpS9jdddddObS4/Oi/h554\n4olG3XPPPeeVV61a5XsMe+h78+bNvbI+TaM29nmhR48eNbYTAD788EOvfOuttxp1X3zxRc6v6Sp7\nGoP9O7HeN3r16uV7HHv6SrYlU/VppKeeeqpRZ2/7eeSRR4ztefPmeWV7udj777/f2C7l8O3QbwRG\nRERERERElFS8aCYiIiIiIiLywYtmIiIiIiIiIh+c01wie+yxh1e+8cYbjbr69et75W+++cao0/dd\nu3ZtSK1LFvtW+Ndff71XLnTOnH0LfX2e1bJl5jJ71157rVfu2rVr1uNma8+wYcO88tdfmzdYfOaZ\nZ7Iel8LRp08fr8w5zcHS+5g+Zw4w5xHb93XYvHlzuA3L4kc/4t+Va2OvAT927FivnO38Z59z9Zzt\nJYT0OY42/b4RANCgQQPfffX2dOzY0ah78MEHvbI9d/3jjz/2PSalNGzY0CvrS0wB5u84AwcONOpy\n7d/nn3++sd2+fXuvPGvWLKPuggsu8Mr33XdfQa/nugMOOMArX3LJJUbdr371K69s92G939pziPX7\nGTRq1Mj3efZx86nbb7/9vLI9p5l2XCbx0EMPzXlffYlA/TwOmMtF2T8T+n2Fpk6datRdddVVXnn2\n7NlGnX6cLl26GHX60nR33nmnUdeqVStjW78vVNj4GwERERERERGRD140ExEREREREfnIe3i2iPQA\nMAhAZwBtAZymlBpr7XMzgF8BaApgFoABSqmFxTc3OeyhBiNGjPDKBx98sO/zrrzySmP7xRdfDLZh\nuZkrIrHNVR/uAQA//vGPfffVh/bMnTvXqLOHiuimTZvmld9//32jbvz48V5ZX2IB2HFYof5zYA9p\nqaio8Mpnn322URf08OwVK1ZkirHONmpRL4lTCG0oaWKyXbRokbF93nnneeVddtnFqMu25EkQNm7c\naGyvXr061NcrQKxz1ZcTAoCDDjrId1/9fLx8+XKjbsCAAV65tvc9vZ+ecMIJRp0+HHefffbJ6Rg2\ne/rALbfc4pVHjRqVtW15inW2+dDfl+2fAX0JrzfeeCOQ18vWT/XXt5fgybZ0ThA++eSTTDHW2eo/\n//aQ+RkzZnjlbFMjgmJPuTv33HN999X7X0RLTE0QkRaI6bWPPZRZHzIPAP/973+98sKFhTXNHjKv\nv2fbQ/2feuqpnI759ttvG9t6zvYydfYyWkOGDPHK9nKWQSvkk+bGAOYBGAhA2ZUi8jsAVwC4FEAX\nAOsATBSR+va+FFvM1THanDJm65imTZtmiszWTczVXczWMdpcXmbrplvBbMtW3hfNSqkJSqkblVIv\nAKjpT7VXAfiTUuolpdT7AH4JYDcAp9WwL8UTc3VMs2bNMkVm65hOnTpliszWTczVXczWMe3atcsU\nma2b/gdmW7YCndMsIu0BtAEwJfOYUmoNgDcAdAvytah0mKu7mK27mK2bmKu7mK27mK27mG35CHrJ\nqTZIDVtYYT2+Il3nNH1JhMcee8yo0+cA2HNxJk+e7JUnTpwYTuOKE3mu+vybOnXq+NbZcy30uZJf\nffWVUTdlyhQUYt26dTWWAWDChAnG9pFHHumVsy1foy//AJhzNvQ5KCGIPFsKTWyzfeutt4ztKOeS\n23Om7XsYxFDkuerLgv3sZz8z6nLN8rPPPjO287l/h36et98z77rrLq98ww03GHX60o82vd36sjaA\nuaSJvRzVkiVLcmhxziLPNlf2kkLZ5qHedtttXnnr1q2BvH6TJk28sr3sWUzFKtvTTtv+Aeg//vEP\no+6jjz6qsRwkvb+dfvrpRp3evz/88EOjLqbLTMUm25EjR5by5QDs+Htvro444giv3LdvX6NOnxut\n93UAuPDCC43tsOcx63j3bCIiIiIiIiIfQV80L0dqrL/9Z7/W6TpKJuaacNrds23MNuHsT740zNZN\nzNVdzDbhtLtn25itu5htmQh0eLZSapGILAfQE8C7ACAiTQB0BfBAkK8VB/aQoOuuuy6n573wwgvG\n9kUXXRRYm8IQh1y1mx3h1FNPzfl5M2fO9MqlWJ5g6NChxrY+1NNejkofnmQv0aEPeQxieHbr1q1R\nXV29w+NxyDZs9nDASZMmeeVevXqVujmBO+aYYzB69OgdHo9ztvYyT3FlDz1+5ZVXommIJg656ssL\n2UOZdfZ0mU2bNnnl22+/PfiGAXjooYe8sj3k+9lnn/XKVVVVvsewh5jr/8eXX37ZqDvkkEO8crFD\nj+OQba7sZYr097CHH37YqKvpvSdIdl72z10pdejQAXPmzNnh8Thnu//++5f8NfXh/XvuuadRp+dp\nnyfCXoKwEHHOttQaNGhgbOtLEl588cVG3d577+2V7amO+hJUffr0MeqiXBaykHWaGwPYF9vvHre3\niBwK4Bul1BcA7gHwBxFZCKAawJ8ALAHwQg2Ho/hirg7Rf1kFs3XKt99+q28yWzcxV3cxW4esX79e\n32S27umY/pfZlqFCPmk+EsA0pCa9KwCZlbQfB3CRUmqYiDQC8HekFvieAeBkpdSmmg5GscRcHbN0\n6dJMkdk6RrsJB7N1E3N1F7N1zAcffJApMls3jQSzLVt5XzQrpf6HWuZCK6WGAhhaWJMoBqqUUm/Z\nDzLX5KqsrMwMkWO2junXr19meDazdRNzdRezdUxVVVVmeDazdVONuQLMthwEveSU85o2beqV7aUu\n7HmpurVr13rlsWPHBt8wx7Vv394rZ5u/tGbNGqPOGpZccq+++qpXtttm30afwmH/DDz++ONe2YU5\nzUlk94WglqEJgn5+Oeuss4y6a6+9ttTNiaVC5yPPnTvXK4e8lB4A4MsvvzS2zzzzTK+sz28Gcp/j\n3KFDB9+6ctKwYUPfugULFhjbYfTvIUOGeGV7DrM+5/GHH34I/LWTyJ63rG/bS06Vgv76HTt2NOrG\njBnjlZ977rmStclFdj/V76GkLx1Ym2XLlnnltm3bGnX6Un72fUD0+er2NdOAAQO88rx584y6OM5d\nB7jkFBEREREREZEvXjQTERERERER+eDw7Dw1btzYKx988MFGXbZhWvoQBX2oNuXmu+++y2m/2bNn\nF/S8sGg34MK4ceOMur59+/o+74QTTvDK+s8csOOt+Sm7unXN09xRRx0VUUso44033jC29eXg/vSn\nPxl1v/nNb7zyli1bwm0YzGHDv/vd74y6iooKY/v7778PvT0ueeyxxyJ9fX249hlnnGHUvfnmm165\nVatWvsew3+f32msvr/zpp58W28TEsJeB0dnLaobBHtKrmzFjhlf+6quvQm9LEkU9/PWJJ57wynaf\n0pd1s+5GTnn6yU9+Ymxfc801Xlmf9lgM/f37tttuM+qmTZvmle1pG0nET5qJiIiIiIiIfPCimYiI\niIiIiMgHL5qJiIiIiIiIfOQ9p1lEegAYBKAzgLYATlNKjdXqHwVwgfW0CUqp3sU0NCotWrQwtvXl\norLNYX799deN7Y0bNwbbsHDNFZFIc7WXY0qvQ1sjPYeePXsadS1btvTKS5YsCah1hRk1apSxrc9p\ntn+W9Dnw+SwL4GfFihWZYuTZlpr9/bv88ssjakk4pk+fnikmJlt7iZhf//rXXtleiuiee+7xyqWY\nE6Xfh2CXXXYx6uz58JMnTw69PUhQrkmi5wwAGzZsKOg4559/vlceOnRovk9PVLatW7f2yvvuu69R\nt2jRIq+8fPny0Nuiv2fa75/2PRNK6ZNPPskUY5Xt/Pnzje0uXbqE/ZJZ7bfffl7Zfj+IuQki0gIJ\nufZ58cUXje0pU6Z45Wz3brDpfezCCy806vSlGX/xi18Ydfqyqy4o5EZgjQHMA/AIgDE++4wH0B9A\n5rucqCtGgt8ZjLkmVP369TPrVTJbxzRt2jRzgyNm6ybm6i5m65hGjRplbkDKbN10K4C/+tQxW8fl\nfdGslJoAYAIAiP9HrRuVUiuLaRhFirk6plmzZli9ejXAbJ3TqVMnfPDBBwCzdRVzdRezdUy7du0y\noxiYrZv+B2ZbtsKa03ysiKwQkfkiMlxEdg3pdai0mKu7mK27mK2bmKu7mK27mK27mK3jwlineTyA\nZwEsArAPUkMZxolIN5WwiQsAcN999xnbhx56qFe2/zv62H17bbRNmzaF0LqSKmmu9tq6uc69yDbP\nPGr5zKku8f/DqT5bG/176+B/z5aIbPV5Vvba6nfffbdX7t07/Olh+nrqMV4jNBG5Jom+hvSNN94Y\nXUMSkq3dlPRoFwDAunXrAn+9Ro0aGdv6/UrstujrccdMbLIt9TrNxxxzjLGd7XccfZ3tBIlNttno\n72nV1dUFHcM+P95yyy1e+bLLLjPqJkyY4JXt+c36fX22bNlSUFtKLfCLZqXU09rmByLyHoBPARwL\nYFqNT6LYY67Jpt0IbAfMNtm0G4HtgNm6ibm6i9kmm3YjsB0wW3cx2/IQ+pJTSqlFAFYB2Le2fSk5\nmGuy6Hc9rQ2zTRb7L/jZMFs3MVd3Mdtk6dChQ877Mlt3MVs3hTE82yAiuwNoDmBZ2K8VFH2ZKXtZ\nBd3mzZuN7WHDhnnlhC0xlbewc7WHaOrLNZ177rlhvCSlJbHPUm7imm220Wtr1qwpYUuAb7/91iu/\n9957Rt3VV19tbM+aNcsrp+9OH4m45pokFRUVBT3PXsonaHHKtkGDBl65cePGRt1uu+0W6mvby781\nbdrUd9/PPvss1LYEJU7Zhm3//fc3tvVz/pgx5kI8YfepUihltp06dfLKX3zxhVGnv5+FRb/euffe\ne426l19+2StPnDjRqHvttde88s9//nOjLq59OO9PmkWksYgcKiKHpR/aO729R7pumIh0FZG9RKQn\ngOcBfAxgov9RKYaYq0OsOfXM1iHWmyKzdRNzdRezdYh1HwRm656O6X+ZbRkqZHj2kQDeBvAmUuvQ\n3QngLQA3AdgKoBOAFwAsAPAPAHMAHKOU2lzj0SiOmKtj0ktgAMzWOdqNNpitm5iru5itY7QbojFb\nN40Esy1bhazT/D9kv9g+qfDmUExUKaXesh5jrglWWVmZuVMis3VMv379MHr0aIDZuoq5uovZOqaq\nqgpz5swBmK2rasoVYLZlIfQ5zUlgL2c0cuRIr3z44YcbdRs2bPDKAwYMMOpeeumlEFpXnrZt22Zs\nT5o0ySvnM6f56ae339DQXgYsjGUxbPq8K31JE5s9p/Ohhx7yyqtXrw68XURx9cILLxjbRxxxhFeu\nU6eOUbd161bf4+hzLA855BCj7qijjvLKp5xyilFXr149r6zPFavJ4MGDvXLEyxRRnvr06WNsX3HF\nFQUd59lnnw2iOYmgLwtT6mU0jz/+eGN71123L4Frt0UbWUUx0aNHD2NbX3Lq+eefL3VzEs2+ZtHn\nDR933HFGXSnmNGfz0UcfeWV73vKIESO88tSpU406/ff1bHekL7XQ755NRERERERElFS8aCYiIiIi\nIiLyweHZAE4//XRj2x7eoJs9e7ZXfvLJJ0NrE5nGjh3rlefNm2fUHXbYYV5ZH/IDAF27dvXKU6ZM\nMepuuOEGr/zKK68E0Uy0bNnS2L7jjju8sj1EVB+SbS9Xoy9flm05HiLX2OfViy++2Cv/8Y9/NOr0\npelOPvlko+7oo4/2yvXr1zfqpk+f7pVvuukmo+7rr7/2yqeddppRN2jQIGP71Vdf3fE/QL6uu+46\nrzxt2jSjbtGiRaG/fmVlpVfu3bu3UacPy7fp5+CrrrrKqNOHLLtO70f2klNh6Nmzp1d+4IEHfPe7\n8847je1PP/00tDZRYbItOaUP4aXa2e91+tTQOH8v9SWmAOCnP/2pVx4/frxRp/d3eyqNPk221PhJ\nMxEREREREZEPXjQTERERERER+cjrollEBovIbBFZIyIrROQ5Eelo7dNARB4QkVUislZEnhGRVn7H\npFiaxGy00gNnAAATiklEQVTdot1NlNk65j//+U+m+Ffm6iT2WXcxW8e8+eabmSLPx25iny1j+c5p\n7gHgPgBz08+9FcDLInKAUiozKfMeACcDOBPAGgAPAHg2/dxY6Nevn7F92223+e5rz1nLZ7mjBBsA\n4JeIUbb63EV7Ttnw4cO98kEHHWTU6XOcq6qqjLqhQ4f6HlO3Zs0aY1uf17XTTjsZdfayUvo8Znu+\ntT6nZ9y4cUbd4sWLfdtTiCZNmmDVqlVADLMN29/+9reomxCqE044IfPzUxcO5Pruu+8a2/pyE5dd\ndpnv8+w+pM+fnTt3rlFnb/uxl+uw5zSXSOz67DvvvOOVDz30UN/97HNex47bf88cOHCgURfU93bP\nPff0yvYyUhdccIFXbt68uVGnt9W+j8QjjzzilR988EGjrsh7TsQu20I1atTIK9v3EMh1eSp9eTkA\nGDNmjFeuqKgw6mbOnOmV77///pzbGbYDDzwQ77//PuDI+bhQnTt3Nrbt5Vvtc0OCRN5n7e9dUpcl\n1X/Pte8tMmrUKK+s358E2PH+RKWU1yfNSqneSqknlVIfKaXeA9AfwJ4AOgOAiDQBcBGA3yql/qeU\nehvAhQCOFpEuwTadQrQQzNYp2i8czNYxu+yyS6Y4FMzVReyz7mK2jtH+kD4UzNVF7LNlrNg5zU0B\nKADfpLc7I/XXNe/PAEqpBQA+B9CtyNei0mK27mK2bqoAc3UV+6y7mK2beD52F/tsmSp4ySlJjQ+4\nB8BMpdSH6YfbANiklFpj7b4iXRcZ7dMY3HzzzUbdzjvv7Pu8u+66y9hetmxZsA2Lr9hmO2PGDGP7\nz3/+s1d++OGHjTp9WQx7SEuPHttHzbz11lu+r7dy5UpjWx+GVtuyG7kOQdLmpZZCbLMNgz5c05bg\nIWI1uQ4O5GoPNbOXKiml9JSGOIhVn9WXApo8ebJRpy8BmM2VV17pe8wRI0bk3BZ9yDUAdOjQwSs3\nbdrU93nZpsukh9d69KXOtm3blnPbchSrbLP58ssvvbI+PBoAunfv7pVPPPFEo+7FF1/0PaY+TN5e\nWkYfkj1r1iyjTl+Kbvny5dmaHRUnzsdhSfhSmpH2Wfs6ZMCAAV65SZMmRp09vTCunn/+eWN7/vz5\nXvnMM8806qIcnl3MOs3DARwIoHttO1LiDAazdRWzddfeADgUzD3ss+5itu7i+dhN7LNlrKDh2SJy\nP4DeAI5VSi3VqpYDqJ8e369rna6jZOgOZuuUFStWZIrM1jFjx47NFC9lrk5in3UXs3WMdpM8no/d\nxD5bxvK+aE5fMJ8K4Dil1OdW9ZsAtgDoqe2/H1IT5l8rop1UWpcyW7fUresNKmG2jtGGudlvzszV\nDeyz7mK2juH52Hnss2Usr+HZIjIcQD8AfQCsE5HW6arVSqkNSqk1IvIIgLtE5FsAawH8DcAspdTs\nIBuer1NPPdUrt2/f3qjLNrcp23xnh/2QpGz//e9/e+V27doZdX/96199n5frfNaWLVsWfAz9Z8me\nq6kvn2MvlxO0devWZYqJyjZsCZ9XBQDo1q1bZo7Prulsyz7XoMTk5yN2fVZfiuuWW24x6rLdn0E/\nX9apU8eo69Spk1cOagmhbOdnO1t9HvMJJ5xg1H311VeBtKcGscs2m82bN3tlfUkYwJzTfPfdd/s+\nz/7ennfeeV7ZXgZMn0N97733GnULFy7Mtdkltc8++2TmY/J8rHHo/iGR91n7fgJ77LGHV7bvJ/DM\nM88Y2zF5T9vBxo0bjW39nNu1a9dSN8dXvnOaL0PqjnGvWI9fCOCJdPm3ALYCeAZAAwATAFxeeBMp\nAhO1MrN1gHbzGmbrmKlTp2aKmWyZq1vYZ93FbB2j3cCI52M3sc+WsbwumpVStQ7nVkptBPCb9Bcl\nU5VSaofbSTPb5KqsrER1dTXAbJ3Tt29fjB49GqghW+bqBPZZdzFbx1RVVWHOnDkAz8euYp8tY8Xc\nPTtR9OFBtQ1P2Lp1q1fWl6+g+LOXnOrVq5dXPumkkwo6Zj7DiuyfLW1YNPr27WvUvfzyywW1h4hK\nY+3atcb2vHnzjO3KysoStiaennvuOWNbXwLq8ccf931eKYZr2udjfRkTe1j5mDFjvLI9VJB2NGHC\nBGNbf6+zp8D997//zemY9nJe11xzjVd+9tln820ixYid7YIFC2osU+30vgYAN9xwg1e2z7kHHXSQ\nsf2Xv/zFK2/atCmE1hVm0KBBxrY+XcdeJjhKBd09m4iIiIiIiKgc8KKZiIiIiIiIyAcvmomIiIiI\niIh8lM2c5n/9619e+Y9//KNRZy99oY/5zzYni+JnzZo1xvYZZ5zhlfX5zYC59MUVV1xR0Ovdd999\nxrY992LLli1e2V5yikrj1ltvNbZ/8pOf5LwvlTf9XhgAsGrVKmO7S5cuXvmhhx4qSZvixp43PHLk\nSK88fvx4o+7KK6/0yvoykABwyCGHFPSaTz75pFH3+efbl1DV5zAD5nJY+rmZ8rd48WJju2PHjl75\ngAMOMOrOP/98r3zggQcadfqyUvZSVfbSOpQcl1xyibFt38NA/z18/fr1JWmTq5544gmvbH+f7fcl\n/bw7ePBgo27GjBle2Z43HQS77+vLrg4YMMCo05eLHTFiROBtKRQ/aSYiIiIiIiLywYtmIiIiIiIi\nIh9S2/JLxs4igwGcDmB/AD8AeBXA75RSH2v7vALgGO1pCsDflVIDfY55BIA38245hekbANPBbJ1R\nv379zPICzNYxdevWzQw1nQbgMuYanPr16xvbs2fPNrbvv/9+r2wvdxcg9ll3MVvH/OhHP8osr1T0\n+Tj9nERmu3z5cmO7efPmxna9evVK2ZwgJarPHn744cb2VVdd5ZX16UUA0KRJE688ceJEo06f2vLD\nDz8YdXvuuadXPvroo406fRpku3btjLqFCxd6Zf29FAAefPBBRKBzTWtw6/L9pLkHgPsAdAXQC0A9\nAC+LyE7aPgrACACtAbQB0BbA9Xm+DkVrAJitU7STIbN1jPamVBfM1UXss+5ito7R5m3yfOwm9tky\nlteNwJRSvfVtEekP4CsAnQHod2xYr5RaWXTrKCoLAfQHs3VGRUVF5iZGzNYxu+yyS6Y4FMBkMFfX\nsM+6i9k6ZqedvOuooeD52EXss2Ws2DnNTZH668o31uPnishKEXlPRP5i/TWGkoHZuovZuqkCzNVV\n7LPuYrZu4vnYXeyzZargJackdV/zewDMVEp9qFWNBLAYwFIAnQAMA9ARwFlFtJNKj9m6i9m66Tow\n10Cl7wPgOeywwyJqCfusw5itm8rufNyyZUuv3KpVK6MuPc/bFYnps2+//bax3b9/f69cUVFh1A0a\nNMgrd+/e3ajTl9+1lwjba6+9vLK+bBVgLvc7a9Yso27SpEle2X6vjati1mkeDuBAAMasb6WUfjeU\nD0RkOYDJItJeKbWoiNej0hkMZusqZuuuvQFU6Q8wVyewz7qL2bqL52M3sc+WsYKGZ4vI/QB6AzhW\nKbWslt3fACAA9i3ktSgS3cFsnbJixYpMkdk6ZuzYsZnipczVSeyz7mK2jnnnnXcyRZ6P3cQ+W8by\n/qQ5fcF8KoAfK6U+z+EphyM19r+2HzCKj0uZrVvq1vW6OrN1jLZs4PJs+6Ux1+Rhn3UXs3VMOZ+P\n9SVs7eHYH374ob17UjnTZ7///ntje8iQIRG1JDnyumgWkeEA+gHoA2CdiLROV61WSm0Qkb0B/ALA\nOABfAzgUwF0A/qeUej+4ZlPIfmC2blm3bl2myGwd061bN0yZMgUAdk1ny1zdwj7rLmbrmH322Qfz\n588HeD52FftsOVNK5fwFYBuArTV8/TJdvzuAVwCsBLAewAIAtwKoyHLMI5D6awy/4vPFbN39YraO\nfYmInS1zdeuLfdbdL2br7lfRuSYt2xYtWnhfW7ZsMb7effdd4yvqtgaQK/use19H1HYdnO86zVnn\nQCullgA4Np9jUixVKaXe0h9gtslWWVmJ6upqgNk6p2/fvhg9ejRgZctcncE+6y5m65iqqirMmTMH\n4PnYVeyzZayYu2cTEREREVGZW7VqlVfW7qNC5IyC7p5NREREREREVA7icNHcMOoG0A6CyoTZxsTG\njRszRWbrmG+++SZTDCIT5ho/7LPuYraO0W66yWzdxFzdVWsmcbhoroy6AbSDypgdh4q0bJm36kFl\nQIcM6jhUpIkTJ2aKlQEcLohjULAqY3YcCk5lzI5DRdKWVqoM6JBBHYeCURmz41BwKmvbQfR11aIg\nIs0BnAigGsCGSBtDDZH6oZmolPq62IMx21hhtu4KLFvmGivss+5itu5itm5iru7KOdvIL5qJiIiI\niIiI4ioOw7OJiIiIiIiIYokXzUREREREREQ+eNFMRERERERE5IMXzUREREREREQ+YnHRLCKXi8gi\nEflBRF4Xkao8nz9ERLZZXx/m8LweIjJWRL5MP6dPDfvcLCJLRWS9iEwSkX3zPY6IPFpD+8ZZ+wwW\nkdkiskZEVojIcyLS0dqngYg8ICKrRGStiDwjIq0KOM4rVlu2isjw2r5fhUhytkHkmt6v6Gzjlmv6\n9UqeLfts/LONss/mcpxyzjaKPpt+nlPZxi3X9OslNtu45JrHcZzP1rU+m8dxEpNtlH02l+O4mG3k\nF80i8nMAdwIYAuBwAO8AmCgiLfI81PsAWgNok/7qnsNzGgOYB2AggB1uIy4ivwNwBYBLAXQBsC7d\ntvr5HCdtvNW+flZ9DwD3AegKoBeAegBeFpGdtH3uAXAKgDMBHANgNwDPFnAcBWCE1p62AK73aXfB\nHMg2iFyBYLKNTa5ApNmyzyYjW56Pcz8Oz8fJyzY2uQJOZBuXXHM9Tjlk61qfzfU4ScuW5+Pcj1N8\ntkqpSL8AvA7gXm1bACwBcH0exxgC4K0i27ENQB/rsaUAfqttNwHwA4Bz8jzOowDG5NmeFuljddde\neyOA07V99kvv0yXX46QfmwbgLmabe7ZB5RpUtlHmGpds2WfjmW1c+iyzDTZXZsvzcSmyjVOuzDa4\nXJlt8NnGpc+WU7aRftIsIvUAdAYwJfOYSv3PJgPolufhOqSHCHwqIk+JyB5Ftq09Un+J0Nu2BsAb\nBbQNAI5NDxmYLyLDRWTXWvZvitRfRb5Jb3cGUNdqzwIAn9fSHvs4GeeKyEoReU9E/mL9NaZoZZRt\nvrkCwWQbSa5AfLNlny1egNnGuc8CZZZtXPtsum0uZOtCnwXinW1Z9Vkgvtk60mdrOk5GkrKNc58F\nHMu2bj47h6AFgDoAVliPr0DqLwm5eh1AfwALkPq4fSiA6SJysFJqXYFta4PUN7ymtrXJ81jjkRpK\nsAjAPgBuBTBORLqlO4lBRASp4QgzlVKZ+QltAGxK//Dm1B6f4wDASACLkfprUicAwwB0BHBWnv+v\nbMoh27xyBYLJNuJcgfhmyz5bvCCyjXOfBcoz27j2WSDh2TrSZ4F4Z1uOfRaIb7aJ7rNZjgMkK9s4\n91nAwWyjvmgOhFJqorb5vojMRuobcw5SwwMipZR6Wtv8QETeA/ApgGORGi5gGw7gQOQ2NyGbzHGO\nttrzsNWe5QAmi0h7pdSiIl8zUHHOtoBcgWCyTXyugHPZss+mxTlXgNkWg9nWKPG5AvHOln22OMy2\nRonPNs65Am5mG/WNwFYB2IrUpGxdawDLCz2oUmo1gI8B7HC3tzwsR2p+QaBtA4B0OKtQQ/tE5H4A\nvQEcq5RaarWnvog0yaU91nGW1dKkN5D6vxbz/bKVXbbZcgWCyTYGuQLxzZZ9tniBZxvnPguUTbZx\n7bNAgrONQa5AGWZbJn0WiG+2ie2zNRzHmWzj3GcBN7KN9KJZKbUZwJsAemYeS3+s3hPAq4UeV0Qq\nkBoKUNs3LFvbFiEViN62Jkjdma3gtqWPszuA5nb70mGfCuA4pdTn1tPeBLDFas9+APYE8Foex6nJ\n4UgNxyj4+2Urx2z9ck3XFZ1tHHIF4pst+2zxwsg2zn02fRzns41rn023LZHZxiFXoDyzLYc+C8Q3\n26T22RyOU5PEZBvnPps+TvKzVSW4Q1y2L6SGEawH8EsA+wP4O4CvAbTM4xh3IHUb8r0A/B+ASUiN\nd29ey/MaAzgUwGFI3WXt6vT2Hun669Nt+RmAQwA8D+ATAPVzPU66bhhSP3B7IRX8XAAfAainHWM4\ngG+Rum16a+2robXPIqSGNnQGMAvADKstWY8DYG8AfwBwRLo9fQAsBDCV2ZrZBpFrUNnGKdcosw0i\nV/bZcLMtJFdmy/NxkrKNU64uZBuXXJmtu33WxWwLyZXZFpdt4D8EBf7gDARQjdQtzV8DcGSezx+N\n1G3af0DqjmqjALTP4Xk/Tge91fr6p7bPUKQmja8HMBHAvvkcB0BDABOQ+svNBgCfAXjQ7hQ+z98K\n4JfaPg2QWodsFYC1AP4DoFU+xwGwO4BXAKxM/58WIDU5v4LZmtkGkWtQ2cYt16iyDSJX9tlwsy0k\nV2bL83GSso1brknPNi65Mlt3+6yL2RaSK7MtLltJH4yIiIiIiIiILFHfCIyIiIiIiIgotnjRTERE\nREREROSDF81EREREREREPnjRTEREREREROSDF81EREREREREPnjRTEREREREROSDF81ERERERERE\nPnjRTEREREREROSDF81EREREREREPnjRTEREREREROSDF81EREREREREPnjRTEREREREROTj/wPJ\nB2IfHM/12gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe137d7f810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(digits[0:8], titles=Y_train.argmax(axis=1)[0:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batchnorm + dropout + data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_model_bn_do():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(1,28,28)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    model = get_model_bn_do()\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=1,\n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    model.optimizer.lr=0.1\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=4,\n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    model.optimizer.lr=0.01\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=12,\n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    model.optimizer.lr=0.001\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=18,\n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_5 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 22s - loss: 0.1446 - acc: 0.9568 - val_loss: 0.0754 - val_acc: 0.9777\n",
      "Epoch 1/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0520 - acc: 0.9837 - val_loss: 0.0434 - val_acc: 0.9877\n",
      "Epoch 2/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0317 - acc: 0.9900 - val_loss: 0.0478 - val_acc: 0.9870\n",
      "Epoch 3/4\n",
      "32000/32000 [==============================] - 21s - loss: 0.0267 - acc: 0.9914 - val_loss: 0.0348 - val_acc: 0.9903\n",
      "Epoch 4/4\n",
      "32000/32000 [==============================] - 21s - loss: 0.0251 - acc: 0.9916 - val_loss: 0.0422 - val_acc: 0.9894\n",
      "Epoch 1/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0182 - acc: 0.9937 - val_loss: 0.0454 - val_acc: 0.9877\n",
      "Epoch 2/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0198 - acc: 0.9933 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 3/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0168 - acc: 0.9942 - val_loss: 0.0352 - val_acc: 0.9906\n",
      "Epoch 4/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0152 - acc: 0.9953 - val_loss: 0.0450 - val_acc: 0.9889\n",
      "Epoch 5/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0142 - acc: 0.9954 - val_loss: 0.0580 - val_acc: 0.9885\n",
      "Epoch 6/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0136 - acc: 0.9952 - val_loss: 0.0403 - val_acc: 0.9912\n",
      "Epoch 7/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0124 - acc: 0.9957 - val_loss: 0.0404 - val_acc: 0.9901\n",
      "Epoch 8/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0455 - val_acc: 0.9898\n",
      "Epoch 9/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0510 - val_acc: 0.9902\n",
      "Epoch 10/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0533 - val_acc: 0.9887\n",
      "Epoch 11/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0126 - acc: 0.9961 - val_loss: 0.0440 - val_acc: 0.9909\n",
      "Epoch 12/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0444 - val_acc: 0.9919\n",
      "Epoch 1/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0540 - val_acc: 0.9904\n",
      "Epoch 2/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0401 - val_acc: 0.9908\n",
      "Epoch 3/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0424 - val_acc: 0.9907\n",
      "Epoch 4/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0550 - val_acc: 0.9907\n",
      "Epoch 5/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0070 - acc: 0.9974 - val_loss: 0.0474 - val_acc: 0.9899\n",
      "Epoch 6/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0373 - val_acc: 0.9925\n",
      "Epoch 7/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0437 - val_acc: 0.9922\n",
      "Epoch 8/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0510 - val_acc: 0.9898\n",
      "Epoch 9/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0409 - val_acc: 0.9920\n",
      "Epoch 10/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0435 - val_acc: 0.9915\n",
      "Epoch 11/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0419 - val_acc: 0.9916\n",
      "Epoch 12/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0356 - val_acc: 0.9922\n",
      "Epoch 13/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0437 - val_acc: 0.9914\n",
      "Epoch 14/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0413 - val_acc: 0.9935\n",
      "Epoch 15/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0462 - val_acc: 0.9920\n",
      "Epoch 16/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0585 - val_acc: 0.9921\n",
      "Epoch 17/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0475 - val_acc: 0.9933\n",
      "Epoch 18/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0440 - val_acc: 0.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_6 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 21s - loss: 0.1448 - acc: 0.9562 - val_loss: 0.0645 - val_acc: 0.9804\n",
      "Epoch 1/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0503 - acc: 0.9840 - val_loss: 0.0421 - val_acc: 0.9876\n",
      "Epoch 2/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0346 - acc: 0.9888 - val_loss: 0.0432 - val_acc: 0.9885\n",
      "Epoch 3/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0219 - acc: 0.9933 - val_loss: 0.0452 - val_acc: 0.9881\n",
      "Epoch 4/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0228 - acc: 0.9929 - val_loss: 0.0429 - val_acc: 0.9888\n",
      "Epoch 1/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0199 - acc: 0.9935 - val_loss: 0.0437 - val_acc: 0.9901\n",
      "Epoch 2/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0171 - acc: 0.9944 - val_loss: 0.0498 - val_acc: 0.9877\n",
      "Epoch 3/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0157 - acc: 0.9952 - val_loss: 0.0420 - val_acc: 0.9897\n",
      "Epoch 4/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0171 - acc: 0.9947 - val_loss: 0.0366 - val_acc: 0.9921\n",
      "Epoch 5/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0408 - val_acc: 0.9908\n",
      "Epoch 6/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0162 - acc: 0.9944 - val_loss: 0.0482 - val_acc: 0.9902\n",
      "Epoch 7/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0393 - val_acc: 0.9915\n",
      "Epoch 8/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0402 - val_acc: 0.9911\n",
      "Epoch 9/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0379 - val_acc: 0.9927\n",
      "Epoch 10/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0119 - acc: 0.9962 - val_loss: 0.0348 - val_acc: 0.9927\n",
      "Epoch 11/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0383 - val_acc: 0.9920\n",
      "Epoch 12/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0485 - val_acc: 0.9910\n",
      "Epoch 1/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0466 - val_acc: 0.9916\n",
      "Epoch 2/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0439 - val_acc: 0.9914\n",
      "Epoch 3/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0455 - val_acc: 0.9923\n",
      "Epoch 4/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0081 - acc: 0.9975 - val_loss: 0.0437 - val_acc: 0.9915\n",
      "Epoch 5/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0454 - val_acc: 0.9904\n",
      "Epoch 6/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0373 - val_acc: 0.9937\n",
      "Epoch 7/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0388 - val_acc: 0.9925\n",
      "Epoch 8/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0425 - val_acc: 0.9929\n",
      "Epoch 9/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0379 - val_acc: 0.9935\n",
      "Epoch 10/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0302 - val_acc: 0.9937\n",
      "Epoch 11/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0454 - val_acc: 0.9920\n",
      "Epoch 12/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0059 - acc: 0.9984 - val_loss: 0.0487 - val_acc: 0.9905\n",
      "Epoch 13/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0401 - val_acc: 0.9922\n",
      "Epoch 14/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0546 - val_acc: 0.9912\n",
      "Epoch 15/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0456 - val_acc: 0.9919\n",
      "Epoch 16/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0512 - val_acc: 0.9901\n",
      "Epoch 17/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0067 - acc: 0.9978 - val_loss: 0.0390 - val_acc: 0.9940\n",
      "Epoch 18/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0411 - val_acc: 0.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_7 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 22s - loss: 0.1554 - acc: 0.9539 - val_loss: 0.0837 - val_acc: 0.9740\n",
      "Epoch 1/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0494 - acc: 0.9850 - val_loss: 0.0689 - val_acc: 0.9802\n",
      "Epoch 2/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0344 - acc: 0.9888 - val_loss: 0.0379 - val_acc: 0.9884\n",
      "Epoch 3/4\n",
      "32000/32000 [==============================] - 21s - loss: 0.0257 - acc: 0.9914 - val_loss: 0.0424 - val_acc: 0.9884\n",
      "Epoch 4/4\n",
      "32000/32000 [==============================] - 21s - loss: 0.0239 - acc: 0.9924 - val_loss: 0.0508 - val_acc: 0.9875\n",
      "Epoch 1/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0188 - acc: 0.9935 - val_loss: 0.0353 - val_acc: 0.9907\n",
      "Epoch 2/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0145 - acc: 0.9952 - val_loss: 0.0520 - val_acc: 0.9856\n",
      "Epoch 3/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0193 - acc: 0.9940 - val_loss: 0.0477 - val_acc: 0.9874\n",
      "Epoch 4/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0162 - acc: 0.9946 - val_loss: 0.0383 - val_acc: 0.9910\n",
      "Epoch 5/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0143 - acc: 0.9948 - val_loss: 0.0373 - val_acc: 0.9904\n",
      "Epoch 6/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0337 - val_acc: 0.9925\n",
      "Epoch 7/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0132 - acc: 0.9954 - val_loss: 0.0571 - val_acc: 0.9882\n",
      "Epoch 8/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0368 - val_acc: 0.9912\n",
      "Epoch 9/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0114 - acc: 0.9959 - val_loss: 0.0404 - val_acc: 0.9913\n",
      "Epoch 10/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0367 - val_acc: 0.9925\n",
      "Epoch 11/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0526 - val_acc: 0.9877\n",
      "Epoch 12/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0100 - acc: 0.9966 - val_loss: 0.0426 - val_acc: 0.9913\n",
      "Epoch 1/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0375 - val_acc: 0.9923\n",
      "Epoch 2/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0358 - val_acc: 0.9935\n",
      "Epoch 3/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0461 - val_acc: 0.9911\n",
      "Epoch 4/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0413 - val_acc: 0.9921\n",
      "Epoch 5/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0413 - val_acc: 0.9916\n",
      "Epoch 6/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0410 - val_acc: 0.9919\n",
      "Epoch 7/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0446 - val_acc: 0.9913\n",
      "Epoch 8/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0323 - val_acc: 0.9927\n",
      "Epoch 9/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0404 - val_acc: 0.9919\n",
      "Epoch 10/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0521 - val_acc: 0.9902\n",
      "Epoch 11/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0053 - acc: 0.9983 - val_loss: 0.0424 - val_acc: 0.9923\n",
      "Epoch 12/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0460 - val_acc: 0.9922\n",
      "Epoch 13/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0461 - val_acc: 0.9922\n",
      "Epoch 14/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0525 - val_acc: 0.9912\n",
      "Epoch 15/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0386 - val_acc: 0.9933\n",
      "Epoch 16/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0442 - val_acc: 0.9926\n",
      "Epoch 17/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0553 - val_acc: 0.9900\n",
      "Epoch 18/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0416 - val_acc: 0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_8 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 22s - loss: 0.1457 - acc: 0.9561 - val_loss: 0.0952 - val_acc: 0.9712\n",
      "Epoch 1/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0516 - acc: 0.9839 - val_loss: 0.0349 - val_acc: 0.9886\n",
      "Epoch 2/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0339 - acc: 0.9890 - val_loss: 0.0358 - val_acc: 0.9906\n",
      "Epoch 3/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0423 - val_acc: 0.9871\n",
      "Epoch 4/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0215 - acc: 0.9930 - val_loss: 0.0513 - val_acc: 0.9884\n",
      "Epoch 1/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0221 - acc: 0.9928 - val_loss: 0.0328 - val_acc: 0.9923\n",
      "Epoch 2/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0167 - acc: 0.9942 - val_loss: 0.0376 - val_acc: 0.9906\n",
      "Epoch 3/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0467 - val_acc: 0.9890\n",
      "Epoch 4/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0181 - acc: 0.9939 - val_loss: 0.0569 - val_acc: 0.9859\n",
      "Epoch 5/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0161 - acc: 0.9948 - val_loss: 0.0430 - val_acc: 0.9906\n",
      "Epoch 6/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0376 - val_acc: 0.9929\n",
      "Epoch 7/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0337 - val_acc: 0.9923\n",
      "Epoch 8/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0474 - val_acc: 0.9908\n",
      "Epoch 9/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0296 - val_acc: 0.9934\n",
      "Epoch 10/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0473 - val_acc: 0.9912\n",
      "Epoch 11/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0477 - val_acc: 0.9895\n",
      "Epoch 12/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0482 - val_acc: 0.9906\n",
      "Epoch 1/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0389 - val_acc: 0.9915\n",
      "Epoch 2/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0077 - acc: 0.9974 - val_loss: 0.0473 - val_acc: 0.9904\n",
      "Epoch 3/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0093 - acc: 0.9972 - val_loss: 0.0581 - val_acc: 0.9877\n",
      "Epoch 4/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0419 - val_acc: 0.9905\n",
      "Epoch 5/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0376 - val_acc: 0.9921\n",
      "Epoch 6/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0468 - val_acc: 0.9922\n",
      "Epoch 7/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0429 - val_acc: 0.9914\n",
      "Epoch 8/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0494 - val_acc: 0.9917\n",
      "Epoch 9/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0470 - val_acc: 0.9918\n",
      "Epoch 10/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0423 - val_acc: 0.9925\n",
      "Epoch 11/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0411 - val_acc: 0.9931\n",
      "Epoch 12/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0480 - val_acc: 0.9910\n",
      "Epoch 13/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0464 - val_acc: 0.9926\n",
      "Epoch 14/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0084 - acc: 0.9977 - val_loss: 0.0537 - val_acc: 0.9913\n",
      "Epoch 15/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0530 - val_acc: 0.9909\n",
      "Epoch 16/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0415 - val_acc: 0.9924\n",
      "Epoch 17/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0516 - val_acc: 0.9914\n",
      "Epoch 18/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0437 - val_acc: 0.9928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_9 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 21s - loss: 0.1571 - acc: 0.9545 - val_loss: 0.0560 - val_acc: 0.9829\n",
      "Epoch 1/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0519 - acc: 0.9843 - val_loss: 0.0498 - val_acc: 0.9862\n",
      "Epoch 2/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0333 - acc: 0.9895 - val_loss: 0.0548 - val_acc: 0.9837\n",
      "Epoch 3/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0269 - acc: 0.9915 - val_loss: 0.0483 - val_acc: 0.9867\n",
      "Epoch 4/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0205 - acc: 0.9929 - val_loss: 0.0464 - val_acc: 0.9865\n",
      "Epoch 1/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0342 - val_acc: 0.9905\n",
      "Epoch 2/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0366 - val_acc: 0.9914\n",
      "Epoch 3/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0164 - acc: 0.9945 - val_loss: 0.0629 - val_acc: 0.9851\n",
      "Epoch 4/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0166 - acc: 0.9945 - val_loss: 0.0427 - val_acc: 0.9897\n",
      "Epoch 5/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0129 - acc: 0.9956 - val_loss: 0.0414 - val_acc: 0.9909\n",
      "Epoch 6/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0143 - acc: 0.9950 - val_loss: 0.0504 - val_acc: 0.9887\n",
      "Epoch 7/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0464 - val_acc: 0.9908\n",
      "Epoch 8/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0363 - val_acc: 0.9912\n",
      "Epoch 9/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0123 - acc: 0.9955 - val_loss: 0.0431 - val_acc: 0.9906\n",
      "Epoch 10/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0340 - val_acc: 0.9922\n",
      "Epoch 11/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0546 - val_acc: 0.9872\n",
      "Epoch 12/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0069 - acc: 0.9978 - val_loss: 0.0354 - val_acc: 0.9905\n",
      "Epoch 1/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0487 - val_acc: 0.9917\n",
      "Epoch 2/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0413 - val_acc: 0.9916\n",
      "Epoch 3/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0090 - acc: 0.9968 - val_loss: 0.0415 - val_acc: 0.9916\n",
      "Epoch 4/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0338 - val_acc: 0.9936\n",
      "Epoch 5/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0407 - val_acc: 0.9920\n",
      "Epoch 6/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0499 - val_acc: 0.9901\n",
      "Epoch 7/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0435 - val_acc: 0.9913\n",
      "Epoch 8/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0388 - val_acc: 0.9915\n",
      "Epoch 9/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0321 - val_acc: 0.9928\n",
      "Epoch 10/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0493 - val_acc: 0.9916\n",
      "Epoch 11/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0543 - val_acc: 0.9883\n",
      "Epoch 12/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0539 - val_acc: 0.9901\n",
      "Epoch 13/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0484 - val_acc: 0.9908\n",
      "Epoch 14/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0068 - acc: 0.9984 - val_loss: 0.0505 - val_acc: 0.9923\n",
      "Epoch 15/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0355 - val_acc: 0.9932\n",
      "Epoch 16/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0498 - val_acc: 0.9920\n",
      "Epoch 17/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0502 - val_acc: 0.9916\n",
      "Epoch 18/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0526 - val_acc: 0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_10 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 22s - loss: 0.1450 - acc: 0.9573 - val_loss: 0.1118 - val_acc: 0.9703\n",
      "Epoch 1/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0527 - acc: 0.9833 - val_loss: 0.0408 - val_acc: 0.9892\n",
      "Epoch 2/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0353 - acc: 0.9885 - val_loss: 0.0305 - val_acc: 0.9904\n",
      "Epoch 3/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0247 - acc: 0.9916 - val_loss: 0.0520 - val_acc: 0.9865\n",
      "Epoch 4/4\n",
      "32000/32000 [==============================] - 22s - loss: 0.0239 - acc: 0.9921 - val_loss: 0.0491 - val_acc: 0.9868\n",
      "Epoch 1/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0311 - val_acc: 0.9925\n",
      "Epoch 2/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0177 - acc: 0.9937 - val_loss: 0.0450 - val_acc: 0.9886\n",
      "Epoch 3/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0186 - acc: 0.9932 - val_loss: 0.0414 - val_acc: 0.9893\n",
      "Epoch 4/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0139 - acc: 0.9953 - val_loss: 0.0385 - val_acc: 0.9913\n",
      "Epoch 5/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0304 - val_acc: 0.9908\n",
      "Epoch 6/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0116 - acc: 0.9960 - val_loss: 0.0431 - val_acc: 0.9905\n",
      "Epoch 7/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0171 - acc: 0.9948 - val_loss: 0.0360 - val_acc: 0.9913\n",
      "Epoch 8/12\n",
      "32000/32000 [==============================] - 21s - loss: 0.0130 - acc: 0.9956 - val_loss: 0.0468 - val_acc: 0.9910\n",
      "Epoch 9/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0082 - acc: 0.9975 - val_loss: 0.0451 - val_acc: 0.9903\n",
      "Epoch 10/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0102 - acc: 0.9962 - val_loss: 0.0445 - val_acc: 0.9909\n",
      "Epoch 11/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0092 - acc: 0.9968 - val_loss: 0.0410 - val_acc: 0.9914\n",
      "Epoch 12/12\n",
      "32000/32000 [==============================] - 22s - loss: 0.0090 - acc: 0.9968 - val_loss: 0.0439 - val_acc: 0.9916\n",
      "Epoch 1/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0077 - acc: 0.9976 - val_loss: 0.0385 - val_acc: 0.9913\n",
      "Epoch 2/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0406 - val_acc: 0.9906\n",
      "Epoch 3/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0069 - acc: 0.9974 - val_loss: 0.0399 - val_acc: 0.9929\n",
      "Epoch 4/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0604 - val_acc: 0.9880\n",
      "Epoch 5/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0399 - val_acc: 0.9922\n",
      "Epoch 6/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0045 - acc: 0.9982 - val_loss: 0.0397 - val_acc: 0.9928\n",
      "Epoch 7/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0352 - val_acc: 0.9927\n",
      "Epoch 8/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0485 - val_acc: 0.9907\n",
      "Epoch 9/18\n",
      "32000/32000 [==============================] - 21s - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0483 - val_acc: 0.9907\n",
      "Epoch 10/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0391 - val_acc: 0.9928\n",
      "Epoch 11/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0434 - val_acc: 0.9914\n",
      "Epoch 12/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0431 - val_acc: 0.9924\n",
      "Epoch 13/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0511 - val_acc: 0.9925\n",
      "Epoch 14/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0061 - acc: 0.9978 - val_loss: 0.0411 - val_acc: 0.9935\n",
      "Epoch 15/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0336 - val_acc: 0.9946\n",
      "Epoch 16/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0367 - val_acc: 0.9936\n",
      "Epoch 17/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0450 - val_acc: 0.9933\n",
      "Epoch 18/18\n",
      "32000/32000 [==============================] - 22s - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0444 - val_acc: 0.9932\n"
     ]
    }
   ],
   "source": [
    "models = [fit_model() for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,m in enumerate(models):\n",
    "    m.save_weights(model_path+'ensmb-'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "evals = np.array([m.evaluate(X_val, Y_val, batch_size=256) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0448,  0.9925])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(X_val, batch_size=256) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10000, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_preds = all_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.9950000047683716, dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.metrics.categorical_accuracy(Y_val, avg_preds).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should have around 99.5% accuracy. Let's see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 28, 28, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.expand_dims(X_test,1)\n",
    "X_test = np.squeeze(X_test,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(X_test, batch_size=256) for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = all_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually got 99.4% accuracy on the test set, not bad!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
